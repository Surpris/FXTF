{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "OHLCを画像で表現し、それを用いてFXTFのデータのトレンドを予測できるか検討する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:25:59.655246Z",
     "start_time": "2017-09-25T13:25:49.772658Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# from FX.FX import SQLAnaforFX\n",
    "from FX.FX.core import drawfigfunc as dff\n",
    "from FX.FX.core import datetimefuncs as dtf\n",
    "from FX.FX.core import analyzefuncs as af\n",
    "# from FX.FX import KerasModelAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:12.499787Z",
     "start_time": "2017-09-25T13:26:12.496781Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basepath = \"C:/Users/Surpris/Desktop/20170918/\"\n",
    "basepath = \"../../images/20170918/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:13.900118Z",
     "start_time": "2017-09-25T13:26:13.711756Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filelist = glob.glob(basepath + \"images-ohlc/*.png\")\n",
    "img = Image.open(filelist[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:40.358924Z",
     "start_time": "2017-09-25T13:26:38.705449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbr_of_img = 6000\n",
    "\n",
    "img1 = np.array(Image.open(filelist[0]))[:,:,0]\n",
    "imglist_shape = (nbr_of_img, img1.shape[0], img1.shape[1], 1)\n",
    "imglist = np.zeros(imglist_shape)\n",
    "for ii in range(nbr_of_img):\n",
    "    img = Image.open(filelist[ii])\n",
    "    buff = np.array(img)[:,:,0][: ,:, None]\n",
    "    imglist[ii] = buff.copy()\n",
    "imglist = imglist.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T16:47:47.716603Z",
     "start_time": "2017-09-21T16:47:45.644360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(basepath + \"ML/imglist.npy\", imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:26:27.289754Z",
     "start_time": "2017-09-22T00:26:27.051736Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imglist = np.load(basepath + \"ML/imglist.npy\")\n",
    "nbr_of_img = len(imglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:42.439977Z",
     "start_time": "2017-09-25T13:26:42.336283Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(basepath + \"FXTF/USDJPY-cd1_20170806_k030.csv\")\n",
    "y = data[[\"label1\", \"label2\", \"label3\"]].as_matrix()[9:nbr_of_img+9].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:27:33.089635Z",
     "start_time": "2017-09-22T00:27:32.912998Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(imglist, y, test_size=0.3, random_state=300)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:45.365490Z",
     "start_time": "2017-09-25T13:26:45.257386Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    img1\n",
    "except:\n",
    "    img1 = imglist[0]\n",
    "input_shape = (img1.shape[0], img1.shape[1], 1)\n",
    "\n",
    "model = Sequential()\n",
    "# Input layer\n",
    "model.add(Conv2D(10, 3, input_shape=input_shape, activation=\"relu\"))\n",
    "# model.add(Activation(\"relu\"))\n",
    "# 2nd layer\n",
    "model.add(Conv2D(10, 3, activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "# 3rd layer()\n",
    "model.add(Conv2D(20, 3, activation=\"relu\"))\n",
    "# model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# 4th layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "# model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "# Output layer\n",
    "model.add(Dense(y.shape[1], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:46.018131Z",
     "start_time": "2017-09-25T13:26:45.971797Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:55.205896Z",
     "start_time": "2017-09-25T13:26:55.002336Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(basepath + \"ML/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:26:58.286605Z",
     "start_time": "2017-09-25T13:26:58.039026Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(basepath + \"ML/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-25T13:27:00.720105Z",
     "start_time": "2017-09-25T13:27:00.705038Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file=basepath+\"ML/test.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:34:04.387709Z",
     "start_time": "2017-09-22T00:28:02.720940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(EarlyStopping(monitor='val_loss', patience=2))\n",
    "callbacks.append(CSVLogger(basepath + \"ML/history.csv\"))\n",
    "# callbacks.append(ModelCheckpoint(filepath=basepath+\"ML/model_ep/ep{epoch:02d}.h5\"))\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=80,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:29:45.124604Z",
     "start_time": "2017-09-22T02:29:44.798859Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_probability(model, testdata):\n",
    "    \"\"\"\n",
    "    Plot predicted probability\n",
    "    \"\"\"\n",
    "    ### Calculate probability\n",
    "    probs = model.predict_proba(X_test, verbose=0).T\n",
    "    labels = [\"high\", \"lose\", \"low\"]\n",
    "\n",
    "    ### Make histograms of each probability\n",
    "    xbins = np.arange(0, 1.0, 0.05)\n",
    "    hists = np.zeros((3, len(xbins)))\n",
    "    for ii in range(len(probs)):\n",
    "        hists[ii, :-1], bins = np.histogram(probs[ii], bins=xbins)\n",
    "        hists[ii] /= hists[ii].sum()\n",
    "    \n",
    "    ### Plot probability of each dataset\n",
    "    dff.makefig(18, 5)\n",
    "    for ii in range(len(probs)):\n",
    "        plt.subplot(1,3,ii + 1)\n",
    "        plt.plot(probs[ii], linewidth=1.2)\n",
    "        dff.arrangefig(xlabel=\"Time index\", ylabel=\"Probability\", title=\"Probability of {}\".format(labels[ii]))\n",
    "        plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ### Plot histogram\n",
    "    dff.makefig(18, 5)\n",
    "    dxbins = np.diff(xbins)[0]\n",
    "    for ii in range(len(hists)):\n",
    "        plt.subplot(1,3,ii + 1)\n",
    "        plt.bar(xbins, hists[ii], width=0.8*dxbins, hold=\"center\", color=\"g\")\n",
    "        dff.arrangefig(ylabel=\"Frequency\")\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(xbins, 1.0 - np.cumsum(hists[ii]), \"r-\", linewidth=1.5)\n",
    "        dff.arrangefig(xlabel=\"Probability\", ylabel=\"Accumulation\", title=\"Hist of {}\".format(labels[ii]))\n",
    "        plt.ylim(0, 1)\n",
    "    #     plt.yscale(\"log\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return\n",
    "\n",
    "def calc_accuracy_above_threshold(model, X, y, threshold=0.5, verbose=0):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model for the datasets\n",
    "    where the predicted probability is above 'threshold'.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### Extract the datasets with the predicted probability above 'threshold'\n",
    "    probs = model.predict_proba(X, verbose=0).T\n",
    "    inds = np.zeros_like(probs, dtype=bool)\n",
    "    for ii in range(0, probs.shape[0]):\n",
    "        inds[ii] = probs[ii] >= threshold\n",
    "    ind_sum = inds.sum(axis=0) > 0\n",
    "    \n",
    "    ### Evaluate the datasets\n",
    "    if inds.sum() == 0:\n",
    "        score = [0, 0]\n",
    "    else:\n",
    "        score = model.evaluate(X[ind_sum], y[ind_sum], verbose=0)\n",
    "    if verbose > 0:\n",
    "        print(\"<# of events over threshold>\")\n",
    "        print(\"[high, lose, low]:\", inds.sum(axis=1), \",total:\", ind_sum.sum())\n",
    "        print('loss=', score[0])\n",
    "        print('accuracy=', score[1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:53:35.151567Z",
     "start_time": "2017-09-22T00:53:10.164949Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plot_probability(model, imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T01:00:30.455734Z",
     "start_time": "2017-09-22T01:00:12.958324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_accuracy_above_threshold(model, X_test, y_test, 0.7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T01:02:43.392844Z",
     "start_time": "2017-09-22T01:01:42.557451Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_accuracy_above_threshold(model, imglist, y, 0.65, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T00:52:37.873128Z",
     "start_time": "2017-09-22T00:52:37.629455Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(basepath + \"ML/model_{}.h5\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを増やす\n",
    "一気に読み込むとメモリが足りないので、次の手順で訓練する。   \n",
    "\n",
    "1. データセットのパスをシャッフルする。\n",
    "1. シャッフルされたデータセットをいくつかのグループに分ける。\n",
    "1. 順番にそれぞれのグループを与えてモデルを訓練する。\n",
    "1. 評価値もそれぞれのグループを与えつつ計算する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:30:02.853912Z",
     "start_time": "2017-09-22T02:30:02.536718Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grouping_dataset(filelist, labelData, nbr_of_grp, \n",
    "                     shuffle=True, seed=None):\n",
    "    \"\"\"\n",
    "    Separate datasets into 'nbr_of_grp' groups.\n",
    "    \"\"\"\n",
    "    if shuffle is True:\n",
    "        if seed is not None and isinstance(seed, int):\n",
    "            np.random.seed(seed)\n",
    "        ind = np.random.permutation(np.arange(0, len(filelist))).astype(\"int32\")\n",
    "    else:\n",
    "        ind = np.arange(0, len(filelist)).astype(\"int32\")\n",
    "    groups_X = []\n",
    "    groups_Y = []\n",
    "    grp_size = len(filelist) // nbr_of_grp\n",
    "    for ii in range(nbr_of_grp-1):\n",
    "        ind_ii = ind[ii*grp_size:(ii+1)*grp_size]\n",
    "        groups_X.append(filelist[ind_ii])\n",
    "        groups_Y.append(labelData[ind_ii])\n",
    "    ind_ii = ind[(ii+1)*grp_size:]\n",
    "    groups_X.append(filelist[ind_ii])\n",
    "    groups_Y.append(labelData[ind_ii])\n",
    "    return groups_X, groups_Y\n",
    "\n",
    "def load_images_from_filelist(filelist):\n",
    "    img1 = np.array(Image.open(filelist[0]))[:,:,0]\n",
    "    imglist_shape = (len(filelist), img1.shape[0], img1.shape[1], 1)\n",
    "    imglist = np.zeros(imglist_shape)\n",
    "    for ii in range(len(filelist)):\n",
    "        img = Image.open(filelist[ii])\n",
    "        buff = np.array(img)[:,:,0][: ,:, None]\n",
    "        imglist[ii] = buff.copy()\n",
    "    imglist = imglist.astype(\"float32\") / 255.0\n",
    "    return imglist\n",
    "\n",
    "def create_model():\n",
    "    pass\n",
    "\n",
    "def train_with_groups(model, groups_Xpath_train, groups_y_train, \n",
    "                      X_test, y_test, \n",
    "                      epochs=80, useCsvLogger=False, useModelCheckPoint=False):\n",
    "    \"\"\"\n",
    "    Train a model with groups of datasets.\n",
    "    \"\"\"\n",
    "    callbacks = []\n",
    "    callbacks.append(EarlyStopping(monitor='val_loss', patience=2))\n",
    "    if useCsvLogger: # TODO: modify so that logging is for each group.\n",
    "        callbacks.append(CSVLogger(basepath + \"ML/history.csv\"))\n",
    "    if useModelCheckPoint:\n",
    "        callbacks.append(ModelCheckpoint(filepath=basepath+\"ML/model_ep/ep{epoch:02d}.h5\"))\n",
    "        \n",
    "    hists = []\n",
    "    scores = []\n",
    "    for ii in range(len(groups_Xpath_train)):\n",
    "        X = load_images_from_filelist(groups_Xpath_train[ii])\n",
    "        y = groups_y_train[ii]\n",
    "        hist = model.fit(X, y, batch_size=100, epochs=80,\n",
    "                         validation_split=0.1, callbacks=callbacks, verbose=0)\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print('Group {0}: loss={1:.4f}, accuracy={2:.4f}'.format(ii, score[0], score[1]))\n",
    "        hists.append(hist)\n",
    "        scores.append(score)\n",
    "    return hists, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:19:51.810983Z",
     "start_time": "2017-09-22T02:19:51.049190Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepath = \"C:/Users/Surpris/Desktop/20170918/\"\n",
    "filelist = np.array(glob.glob(os.path.join(basepath, \"images-ohlc/*.png\")))\n",
    "data = pd.read_csv(basepath + \"FXTF/USDJPY-cd1_20170806_k030.csv\")\n",
    "y = data[[\"label1\", \"label2\", \"label3\"]].as_matrix()[9:-1].copy()\n",
    "\n",
    "Xpath_train, Xpath_test, y_train, y_test = train_test_split(filelist, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:20:21.122841Z",
     "start_time": "2017-09-22T02:20:06.978681Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grpX, grpY = grouping_dataset(Xpath_train, y_train, 8)\n",
    "len(Xpath_train), len(grpX), len(y_train), len(grpY), [len(q) for q in grpX]\n",
    "\n",
    "X_test = load_images_from_filelist(Xpath_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:28:42.026849Z",
     "start_time": "2017-09-22T02:20:56.625217Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(basepath + \"ML/model.h5\")\n",
    "hists, scores = train_with_groups(model, grpX, grpY, X_test, y_test, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:32:21.988956Z",
     "start_time": "2017-09-22T02:30:05.663839Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_probability(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-22T02:41:46.859230Z",
     "start_time": "2017-09-22T02:38:49.673935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calc_accuracy_above_threshold(model, X_test, y_test, threshold=0.63, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "CNNを用いたFXの予測を行った。   \n",
    "シンプルに入出力層＋３層のモデルを構築してみたが、ただ数値を入れるよりは精度が良くなっているかもしれないという印象である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
